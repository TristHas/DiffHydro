{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36bc63c2-f6bc-4c9d-8e42-499931b21086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import torch\n",
    "#from imports import *\n",
    "\n",
    "sys.path.insert(0, \"./DiffHydro\")\n",
    "sys.path.insert(0, \"./DiffRoute\")\n",
    "\n",
    "from diffhydro import (TimeSeriesThDF, CatchmentInterpolator, StagedCatchmentInterpolator,\n",
    "                       RivTree, RivTreeCluster)\n",
    "from diffhydro.utils import nse_fn\n",
    "from diffhydro.pipelines import LearnedRouter\n",
    "\n",
    "from diffroute.io import _read_rapid_graph\n",
    "from diffroute.graph_utils import define_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3813b3f9-9c59-42cb-8d73-1a2b4491932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('/data_prediction005/SYSTEM/prediction002/home/tristan/data/geoflow')\n",
    "root_discharge = root / \"retro_parquet\"\n",
    "\n",
    "def load_rapid_graph_with_attributes(root, vpu, plength_thr=None, node_thr=None):\n",
    "\n",
    "    g = _read_rapid_graph(root / 'data' / 'configs' / vpu)[0]\n",
    "    df = gpd.read_file(root / \"tdxhydro\" / f\"streams_{vpu}.gpkg\").set_index(\"LINKNO\")\n",
    "    \n",
    "    params = pd.DataFrame({ \n",
    "        \"is_lake\": df[\"musk_x\"]==.01,\n",
    "        \"dist\": df[\"LengthGeodesicMeters\"],\n",
    "        \"upa\": np.sqrt(df[\"DSContArea\"])\n",
    "    }).astype(\"float32\")\n",
    "\n",
    "    # Standardize\n",
    "    params[[\"dist\", \"upa\"]] = (params[[\"dist\", \"upa\"]] \\\n",
    "                             - params[[\"dist\", \"upa\"]].mean()) \\\n",
    "                             / params[[\"dist\", \"upa\"]].std()\n",
    "    \n",
    "    if (plength_thr is not None) and (node_thr is not None):\n",
    "        clusters_g, node_transfer = define_schedule(g, plength_thr=plength_thr, \n",
    "                                                    node_thr=node_thr)\n",
    "        g = RivTreeCluster(clusters_g, \n",
    "                           node_transfer,\n",
    "                           include_index_diag=True,\n",
    "                           param_df=params)\n",
    "        for g_ in g: g_.irf_fn = \"muskingum\"\n",
    "    else:\n",
    "        g = RivTree(g,\n",
    "                    include_index_diag=True,\n",
    "                    param_df=params)\n",
    "        g.irf_fn = \"muskingum\"\n",
    "    return g\n",
    "\n",
    "def load_vpu(root, vpu, \n",
    "             runoff=None, \n",
    "             interp_df=None, \n",
    "             device=\"cpu\", \n",
    "             plength_thr=10**4, \n",
    "             node_thr=10**4):\n",
    "    if interp_df is None:\n",
    "        interp_df = pd.read_pickle(root / \"data\" / \"interp_weight.pkl\").set_index(\"river_id\")\n",
    "    if runoff is None:\n",
    "        runoff = pd.read_feather(root / \"data\" / \"daily_sparse_runoff.feather\").loc[:\"2019\"] / (3600. * 24)\n",
    "\n",
    "    g = load_rapid_graph_with_attributes(vpu, \n",
    "                                         plength_thr=plength_thr, \n",
    "                                         node_thr=node_thr).to(device)\n",
    "\n",
    "    interp_df = interp_df.loc[g.nodes]\n",
    "    pix_idxs  = interp_df[\"pixel_idx\"].unique()\n",
    "\n",
    "    runoff = TimeSeriesThDF.from_pandas(runoff[pix_idxs]).to(device)\n",
    "    ci = CatchmentInterpolator(g, runoff, interp_df).to(device)\n",
    "    runoff = ci.interpolate_runoff(runoff)\n",
    "    \n",
    "    q = TimeSeriesThDF.from_pandas(pd.read_parquet(root_discharge / f\"{vpu}.pqt\")).to(device)[g.nodes]\n",
    "    \n",
    "    return g, q, runoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fb0942-4b22-45a5-8a72-86e3acd74a99",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class RoutingDataset():\n",
    "    def __init__(self, runoff, discharge, sample_len):\n",
    "        self.runoff = runoff\n",
    "        self.discharge = discharge \n",
    "        self.sample_len = sample_len\n",
    "        self.total_len = self.runoff.shape[0]\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd48f965-8e45-472e-8891-8c3830c345e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7801fc2-e1ed-4b1e-9502-17025895b136",
   "metadata": {},
   "source": [
    "### New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3981f855-743a-4b3c-8e55-1d164bdbe8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plength_thr=10**4\n",
    "node_thr=10**4\n",
    "\n",
    "model_name=\"muskingum\"\n",
    "time_window = max_delay = 30\n",
    "dt = 1/24\n",
    "epochs = 500\n",
    "\n",
    "device = \"cuda:0\"\n",
    "tr_vpu = \"603\"\n",
    "te_vpu = \"602\"\n",
    "n_iter_per_cluster = 100\n",
    "n_clusters = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e5ae67-f5de-424c-9031-cd3ffded2c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 159 ms, sys: 792 ms, total: 951 ms\n",
      "Wall time: 951 ms\n",
      "CPU times: user 56.7 s, sys: 2min 8s, total: 3min 4s\n",
      "Wall time: 23.6 s\n"
     ]
    }
   ],
   "source": [
    "%time interp_df = pd.read_pickle(root / \"data\" / \"interp_weight.pkl\").set_index(\"river_id\")\n",
    "%time runoff = pd.read_feather(root / \"data\" / \"daily_sparse_runoff.feather\").loc[:\"2019\"] / (3600. * 24)\n",
    "runoff = TimeSeriesThDF.from_pandas(runoff).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cddd44d-ec7f-4d79-9a56-66ee75cd022a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Upstream stats computations ... ####\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bde10ecd3e94b79ba9da68a4751251e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing breakpoints:   0%|          | 0/55576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Segmentation into subgraphs ... ####\n",
      "Removing edges...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91454841fdf421fa9a2b68468b7da4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment graph into connected components....\n",
      "Build subgraphs for each cluster and node-cluster map...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d076ee94454a40954176734107c35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Establish dependencies between clusters...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4080550a91f44653a7f8518159f3b736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Grouping subgraphs to cluster and infering dependencies ... ####\n",
      "Initialize dependencies...\n",
      "Associate clusters for remaining subgraphs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1073ec2833a04e7cb52ef84ad88a3f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging graphs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979836df97cd4a58bd7d6e4894219f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing merged graphs node idxs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3fc97473aa4204be5b9f29df97f4ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match breakpoint nodes across clusters...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b63613efeda48edb203b1a9e27713c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Upstream stats computations ... ####\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea460db2bbf4bd28fc92300e0452ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing breakpoints:   0%|          | 0/29581 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Segmentation into subgraphs ... ####\n",
      "Removing edges...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f0744b4d8484ffa918932e9f9ef9a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29581 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment graph into connected components....\n",
      "Build subgraphs for each cluster and node-cluster map...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdce8feeb28b4f4895a90ffc8438e469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/403 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Establish dependencies between clusters...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8995bd992214ec9bbda5ded87c05440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Grouping subgraphs to cluster and infering dependencies ... ####\n",
      "Initialize dependencies...\n",
      "Associate clusters for remaining subgraphs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f308c999954a97b110a8729a38bec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging graphs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0a29136d2e40e888fb7f02fab5bb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing merged graphs node idxs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a639846e1a28464da9f8e9ba28429bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match breakpoint nodes across clusters...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068e13aacb1c478fb1ecd3c1807b81aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tr_g, tr_discharge, tr_runoff = load_vpu(tr_vpu, runoff, interp_df, device,\n",
    "                                         plength_thr=plength_thr, node_thr=node_thr)\n",
    "te_g, te_discharge, te_runoff = load_vpu(te_vpu, runoff, interp_df, device,\n",
    "                                         plength_thr=plength_thr, node_thr=node_thr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45fd1b0-178d-4b15-9ad8-7589a6cd5405",
   "metadata": {},
   "source": [
    "### Small graph, single forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d52ebbcd-94cc-42f4-aeb8-b8cb26add9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd4f90b4-b81b-4f30-afdb-41d823278527",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LearnedRouter(max_delay, dt).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdd601dc-d062-4e56-98ba-b33e99b9f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "te_y = te_discharge\n",
    "te_x = te_runoff\n",
    "\n",
    "tr_y = tr_discharge\n",
    "tr_x = tr_runoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64c3e9c5-faba-4161-afb8-6b244702037f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c57f0d3faf9f4a72bc5290abd9bdf214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 6.05 GiB. GPU 0 has a total capacity of 79.15 GiB of which 988.88 MiB is free. Process 4052476 has 702.00 MiB memory in use. Process 4052540 has 700.00 MiB memory in use. Process 4052521 has 700.00 MiB memory in use. Process 4052468 has 702.00 MiB memory in use. Process 4052491 has 702.00 MiB memory in use. Process 4052501 has 702.00 MiB memory in use. Process 4052545 has 700.00 MiB memory in use. Process 4052505 has 702.00 MiB memory in use. Process 4052510 has 700.00 MiB memory in use. Process 4052535 has 702.00 MiB memory in use. Process 4052525 has 702.00 MiB memory in use. Process 4052515 has 702.00 MiB memory in use. Process 4052552 has 700.00 MiB memory in use. Process 4052530 has 700.00 MiB memory in use. Process 4052601 has 702.00 MiB memory in use. Process 4052558 has 702.00 MiB memory in use. Process 4052576 has 702.00 MiB memory in use. Process 4052484 has 702.00 MiB memory in use. Process 4052597 has 702.00 MiB memory in use. Process 4052589 has 702.00 MiB memory in use. Process 749443 has 1.09 GiB memory in use. Including non-PyTorch memory, this process has 63.26 GiB memory in use. Of the allocated memory 59.24 GiB is allocated by PyTorch, and 3.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[32m      4\u001b[39m     out = model(tr_x[:,\u001b[33m\"\u001b[39m\u001b[33m1940\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33m1950\u001b[39m\u001b[33m\"\u001b[39m], tr_g)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     tr_nse = nse_fn(out.values, \u001b[43mtr_y\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1940\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1950\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.values).mean()\n\u001b[32m      8\u001b[39m     opt.zero_grad()\n\u001b[32m      9\u001b[39m     loss = \u001b[32m1\u001b[39m-tr_nse\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data_prediction005/SYSTEM/prediction002/home/tristan/workspace/rivers/diffroute_reviews/DiffHydro/diffhydro/structs/time_series.py:160\u001b[39m, in \u001b[36mTimeSeriesThDF.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    157\u001b[39m cols  = \u001b[38;5;28mself\u001b[39m._columns[col_key]\n\u001b[32m    158\u001b[39m times = \u001b[38;5;28mself\u001b[39m._index[time_key]\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m V = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m]\u001b[49m[:, :, times.values]  \u001b[38;5;66;03m# [B,C',T']\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m TimeSeriesThDF(V, columns=cols.index, index=times.index)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 6.05 GiB. GPU 0 has a total capacity of 79.15 GiB of which 988.88 MiB is free. Process 4052476 has 702.00 MiB memory in use. Process 4052540 has 700.00 MiB memory in use. Process 4052521 has 700.00 MiB memory in use. Process 4052468 has 702.00 MiB memory in use. Process 4052491 has 702.00 MiB memory in use. Process 4052501 has 702.00 MiB memory in use. Process 4052545 has 700.00 MiB memory in use. Process 4052505 has 702.00 MiB memory in use. Process 4052510 has 700.00 MiB memory in use. Process 4052535 has 702.00 MiB memory in use. Process 4052525 has 702.00 MiB memory in use. Process 4052515 has 702.00 MiB memory in use. Process 4052552 has 700.00 MiB memory in use. Process 4052530 has 700.00 MiB memory in use. Process 4052601 has 702.00 MiB memory in use. Process 4052558 has 702.00 MiB memory in use. Process 4052576 has 702.00 MiB memory in use. Process 4052484 has 702.00 MiB memory in use. Process 4052597 has 702.00 MiB memory in use. Process 4052589 has 702.00 MiB memory in use. Process 749443 has 1.09 GiB memory in use. Including non-PyTorch memory, this process has 63.26 GiB memory in use. Of the allocated memory 59.24 GiB is allocated by PyTorch, and 3.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(range(n_iter_per_cluster), desc=\"Training\")\n",
    "\n",
    "for _ in pbar:\n",
    "    out = model(tr_x[:,\"1940\":\"1950\"], tr_g)\n",
    "    \n",
    "    tr_nse = nse_fn(out.values, tr_y[:,\"1940\":\"1950\"].values).mean()\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss = 1-tr_nse\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    out = model(te_x, te_g)\n",
    "    te_nse = nse_fn(out.values, te_y.values).mean()\n",
    "    \n",
    "    pbar.set_postfix({\"Tr NSE:\": tr_nse.item(), \"Te NSE\":te_nse.item()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c13ead7-ed03-4efb-b400-90b3218bdf2e",
   "metadata": {},
   "source": [
    "### Large one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a54d3f-3b2b-4f0e-b445-d83ce04ec6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_idx in tqdm(range(n_clusters)):\n",
    "\n",
    "    te_y = te_discharge\n",
    "    te_x = te_runoff\n",
    "    \n",
    "    pbar = tqdm(range(n_iter_per_cluster), desc=\"Training\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        transfer_bucket = model.init_upstream_discharges(tr_runoff, tr_g, cluster_idx)\n",
    "        te_transfer_bucket = model.init_upstream_discharges(te_runoff, te_g, cluster_idx)\n",
    "    \n",
    "    g = tr_g[cluster_idx]\n",
    "    tr_y = tr_discharge[g.nodes]\n",
    "    tr_x = tr_runoff[g.nodes]\n",
    "\n",
    "    for _ in pbar:\n",
    "        out = model.route_one_cluster(tr_x, tr_g, cluster_idx, transfer_bucket)\n",
    "        \n",
    "        tr_nse = nse_fn(out.values, tr_y.values).mean()\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss = 1-tr_nse\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        out = model(te_x, te_g)\n",
    "        te_nse = nse_fn(out.values, te_y.values).mean()\n",
    "        \n",
    "        pbar.set_postfix({\"Tr NSE:\": tr_nse.item(), \"Te NSE\":te_nse.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6825b646-2d5d-473a-855f-68fd164d201d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4cf9ce-84c6-4181-b59d-2e5bf23b966b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
